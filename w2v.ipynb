{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import brown\n",
    "#for sent in brown.sents():\n",
    "#    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['rock cab',\n",
    "'ren cab',\n",
    "'us cab',\n",
    "'howagirepiguros.com wig',\n",
    "'stochost street',\n",
    "'skipholt street',\n",
    "'11.02.2010.v.m ski',\n",
    "'bansko ski',\n",
    "'2008 paddle',\n",
    "'gf paddle',\n",
    "'olivier trolleybus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rock', 'cab'], ['ren', 'cab'], ['us', 'cab'], ['howagirepiguros.com', 'wig'], ['stochost', 'street'], ['skipholt', 'street'], ['11.02.2010.v.m', 'ski'], ['bansko', 'ski'], ['2008', 'paddle'], ['gf', 'paddle'], ['olivier', 'trolleybus']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'rock', 1: 'cab', 2: 'ren', 3: 'us', 4: 'howagirepiguros.com', 5: 'wig', 6: 'stochost', 7: 'street', 8: 'skipholt', 9: '11.02.2010.v.m', 10: 'ski', 11: 'bansko', 12: '2008', 13: 'paddle', 14: 'gf', 15: 'olivier', 16: 'trolleybus'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print idx2word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "#print idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([18.695258398060602])\n",
      "set([15.113533755922452])\n",
      "set([12.032077604045215])\n",
      "set([9.530654434772838])\n",
      "set([7.4193262688607895])\n",
      "set([5.7225959475763375])\n",
      "set([4.394397540369854])\n",
      "set([3.3934644777208716])\n",
      "set([2.5961881937797773])\n",
      "set([1.9698766886444488])\n",
      "set([1.495507491703558])\n",
      "set([1.1377430367124775])\n",
      "set([0.8979108029783344])\n",
      "set([0.7488006034545833])\n",
      "set([0.6421889426882527])\n",
      "set([0.5522812234822513])\n",
      "set([0.47971205399980216])\n",
      "set([0.43656587660635443])\n",
      "set([0.4171381910723126])\n",
      "set([0.40719163159585814])\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 100\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data[0]\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print({loss_val/len(idx_pairs)})\n",
    "        #print len(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = W1\n",
    "u = W2\n",
    "def cosine_distance(u, v):\n",
    "    return numpy.dot(u, v) / (math.sqrt(numpy.dot(u, u)) * math.sqrt(numpy.dot(v, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosdis(v, u):\n",
    "    # which characters are common to the two words?\n",
    "    common = v[1].intersection(u[1])\n",
    "    \n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v[0][ch]*u[0][ch] for ch in common)/v[2]/v[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    " #print cosdis('1','2')\n",
    " #print cosine_distance('2323','3232')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    from collections import Counter\n",
    "    from math import sqrt\n",
    "\n",
    "    # count the characters in word\n",
    "    cw = Counter(word)\n",
    "    # precomputes a set of the different characters\n",
    "    sw = set(cw)\n",
    "    # precomputes the \"length\" of the word vector\n",
    "    lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "    # return a tuple\n",
    "    return cw, sw, lw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosdis(v, u):\n",
    "    # which characters are common to the two words?\n",
    "    common = v[1].intersection(u[1])\n",
    "    \n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v[0][ch]*u[0][ch] for ch in common)/v[2]/v[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = 'safasfeqefscwaeeafweeaeawaw'\n",
    "#b = 'tsafdstrdfadsdfdswdfafdwaed'\n",
    "#c = 'optykop;lvhopijresokpghwji7'\n",
    "a ='cab'\n",
    "b='street'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = word2vec(a)\n",
    "vb = word2vec(b)\n",
    "vc = word2vec(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print cosdis(va,vb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
